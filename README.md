# Composing LLamas

llama.cpp as "raw" inference engine; 
specifically tailored to be run containerized via docker-compose on the NVIDIA  platform.